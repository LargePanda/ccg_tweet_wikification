{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Mention Detection</h1> \n",
    "<h4 align=\"center\">Jiarui Xu - jxu57@illinois.edu</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pyprind\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"/Volumes/backup/ccg_tweet_wikifier_data/\"\n",
    "wikidata_file = \"/Volumes/backup/ccg_tweet_wikifier_data/wikidata/wikidata-20160404-all.json\"\n",
    "entity_alias_output_file = data_folder+\"wikidata/entity_alias.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Models: 5/5                                                            \n"
     ]
    }
   ],
   "source": [
    "from corenlp import *\n",
    "corenlp = StanfordCoreNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Wikidata Alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use wikidata to build lexicon, if a n-gram exists in lexicon, then we consider it a mention candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Wikidata json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_by_tab(dic):\n",
    "    \"\"\"\n",
    "    Join each items in input\n",
    "    \"\"\"\n",
    "    val = \"\"\n",
    "    val += dic.keys()[0] # english label\n",
    "    val += \"\\t\"\n",
    "    val += \"\\t\".join(dic.values()[0]) # aliases\n",
    "    val += \"\\n\"\n",
    "    return val\n",
    "\n",
    "def find_en_aliases(entity):\n",
    "    \"\"\"\n",
    "    Return a list [label, alias_0, alias_1 ... ] for a given entity\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = {}\n",
    "    entity_id = entity[u'id']\n",
    "    \n",
    "    try:\n",
    "        ret[entity_id]= [entity[u'labels'][u'en'][u'value']]\n",
    "    except:\n",
    "        ret[entity_id] = [\"NONE_EN_LABEL\"]\n",
    "    \n",
    "    try:\n",
    "        ret[entity_id].extend([element['value'] for element in entity[u'aliases'][u'en']])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def load_wikidata(wikidata_file, output_file):\n",
    "    \n",
    "    line_count = 20951710    # line count of 04_04 wikidata\n",
    "    \n",
    "    # for progress bar\n",
    "    bar = pyprind.ProgBar(line_count, width=70, monitor = True)\n",
    "    \n",
    "    # set up error statistics\n",
    "    errors = {}\n",
    "    json_errors = []\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # write to file\n",
    "    with open(output_file, \"w\") as g:\n",
    "        with open(wikidata_file, \"rb\") as f:\n",
    "            for line in f:\n",
    "                \n",
    "                # update progress bar\n",
    "                bar.update()\n",
    "                \n",
    "                try:\n",
    "                    # load entity from the line being reading\n",
    "                    entity_content = json.loads(line.strip()[:-1])\n",
    "                    try:\n",
    "                        # get aliases and connect them by tab\n",
    "                        output = join_by_tab(find_en_aliases(entity_content))\n",
    "                        g.write(output.encode('utf8'))\n",
    "                    except:\n",
    "                        errors[entity_content[u'id']] = sys.exc_info()[0]\n",
    "                except:\n",
    "                    json_errors.append(sys.exc_info()[0])\n",
    "\n",
    "    print json_errors, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load Wikidata and extract aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unblock to load\n",
    "\n",
    "# load_wikidata(wikidata_file, entity_alias_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_alias_file = entity_alias_output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file_formate is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wikidata_id    label    alias_1 .... alias_n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### b. Build Alias Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alias_entity_file = data_folder + \"wikidata/alias_entity.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) reverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_mapping(src_file):\n",
    "    \"\"\"\n",
    "    Build a mapping from aliasn to entity_list\n",
    "    \"\"\"\n",
    "    # for progress bar\n",
    "    line_count = 20951708\n",
    "    bar = pyprind.ProgBar(line_count, width=70, monitor = True)\n",
    "    \n",
    "    a2e = {}\n",
    "    \n",
    "    with open(src_file, \"rb\") as f:\n",
    "        for line in f:\n",
    "            bar.update()\n",
    "            segments = line.strip().split(\"\\t\")\n",
    "            entity = segments[0]\n",
    "            for seg in segments[1:]:\n",
    "                if seg not in a2e:\n",
    "                    a2e[seg] = set()\n",
    "                a2e[seg].add(entity)\n",
    "\n",
    "    return a2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                                                                  100%\n",
      "[######################################################################] | ETA: 00:00:00"
     ]
    }
   ],
   "source": [
    "alias_to_entity = reverse_mapping(entity_alias_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_alias_output_txt_file = data_folder+\"wikidata/alias2entity.txt\"\n",
    "\n",
    "bar = pyprind.ProgBar(len(alias_to_entity, width=70, monitor = True)\n",
    "    \n",
    "with open(entity_alias_output_txt_file, \"wb\") as f:\n",
    "    for key in alias_to_entity.keys():\n",
    "        bar.update()\n",
    "        line = [key]\n",
    "        line.extend(alias_to_entity[key])\n",
    "        \n",
    "        text = \"\\t\".join(line)\n",
    "        \n",
    "                      \n",
    "                      \n",
    "                      entity = segments[0]\n",
    "        for seg in segments[1:]:\n",
    "        if seg not in a2e:\n",
    "            a2e[seg] = set()\n",
    "        a2e[seg].add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_alias_output_file = data_folder+\"wikidata/alias2entity.pickle\"\n",
    "\n",
    "with open(entity_alias_output_file, \"wb\") as f:\n",
    "    pickle.dump(alias_to_entity, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alias_to_entity_lower = {}\n",
    "for als in alias_to_entity.keys():\n",
    "    als_lower = als.lower()\n",
    "    if als_lower in alias_to_entity_lower:\n",
    "        alias_to_entity_lower[als_lower] |= alias_to_entity[als]\n",
    "    else:\n",
    "        alias_to_entity_lower[als_lower] = alias_to_entity[als]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump the mapping to file\n",
    "entity_alias_output_file = data_folder+\"wikidata/alias2entity_lower.pickle\"\n",
    "\n",
    "with open(wikidata_file + \"alias2entity.pickle\", \"wb\") as f:\n",
    "    pickle.dump(alias_to_entity_lower, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Experiment with Golden Standards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tweet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(with_grams).pickle\", \"rb\") as f:\n",
    "    tweet_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('en')\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "from stop_words import safe_get_stop_words\n",
    "\n",
    "stop_words = safe_get_stop_words('unsupported language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) First experiment: How n-grams match with our golden mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special(text):\n",
    "    if text[0] in ['$', '#', \"@\"]:\n",
    "        try:\n",
    "            return text[1:]\n",
    "        except:\n",
    "            return text\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment_gram_matching(tweets):\n",
    "    total = 0\n",
    "    match = 0\n",
    "    try:\n",
    "        for tweet in tweets.values():\n",
    "            goldens = tweet['goldens']\n",
    "            for g in goldens:\n",
    "                total += 1\n",
    "                mention = g['mention']\n",
    "\n",
    "                gram_set = set()\n",
    "                for grams in tweet['ngrams'].values():\n",
    "                    for gram in grams:\n",
    "                        gram_set.add(remove_special(gram))\n",
    "\n",
    "                if mention in gram_set:\n",
    "                    match += 1\n",
    "                else:\n",
    "                    # pass\n",
    "                    print tweet['tweet_info']['id']\n",
    "                    print tweet['tweet_info']['text']\n",
    "                    print \"MENTION:\", mention\n",
    "                    print(type(mention))\n",
    "                    print \"======\"\n",
    "    except:\n",
    "        print tweet['tweet_info']['id']\n",
    "    return [match, total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = experiment_gram_matching(tweet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in tweet_corpus.values():\n",
    "    tweet[\"gram_set\"] = set()\n",
    "    for gram_set in tweet[\"ngrams\"].values():\n",
    "        tweet[\"gram_set\"] |= set(gram_set)\n",
    "    tweet[\"mention_set\"] = set([item['mention'].lower() for item in tweet['goldens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = {\"tp\":0., \"fp\":0., \"tn\":0., \"fn\":0.}\n",
    "\n",
    "for tweet in tweet_corpus.values():\n",
    "    for gram in tweet['gram_set']:\n",
    "        gram_low = gram.lower()\n",
    "        if len(gram_low) < 2:\n",
    "            continue\n",
    "        if gram_low in stop_words:\n",
    "            continue\n",
    "        if gram_low in alias_to_entity_lower:\n",
    "            if gram_low in tweet['mention_set']:\n",
    "                stats['tp'] +=1\n",
    "            else:\n",
    "                stats['fp'] +=1\n",
    "                print gram_low\n",
    "        else:\n",
    "            if gram_low in tweet['mention_set']:\n",
    "                stats['fn'] +=1\n",
    "            else:\n",
    "                stats['tn'] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_upper(text):\n",
    "    for c in text:\n",
    "        if c.isupper():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweet_corpus.values():\n",
    "    tweet[\"gram_set\"] = set()\n",
    "    for gram_set in tweet[\"ngrams\"].values():\n",
    "        tweet[\"gram_set\"] |= set(gram_set)\n",
    "    tweet[\"mention_set\"] = set([item['mention'] for item in tweet['goldens']])\n",
    "    \n",
    "stats = {\"tp\":0., \"fp\":0., \"tn\":0., \"fn\":0.}\n",
    "\n",
    "for tweet in tweet_corpus.values():\n",
    "    print \"=======\"\n",
    "    print tweet['tweet_info']['id']\n",
    "    print tweet['tweet_info']['text']\n",
    "    print tweet['mention_set']\n",
    "    for gram in tweet['gram_set']:\n",
    "        gram_low = gram\n",
    "        if len(gram_low) <= 3:\n",
    "            continue\n",
    "        if gram_low in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # if check_upper(gram_low) == False:\n",
    "            \n",
    "        elif gram_low in alias_to_entity and check_upper(gram_low):\n",
    "            \n",
    "            if gram_low in tweet['mention_set']:\n",
    "                stats['tp'] +=1\n",
    "            else:\n",
    "                stats['fp'] +=1\n",
    "                print gram_low\n",
    "        else:\n",
    "            if gram_low in tweet['mention_set']:\n",
    "                stats['fn'] +=1\n",
    "            else:\n",
    "                stats['tn'] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision = stats['tp']/(stats['tp']+stats['fp'])\n",
    "recall = stats['tp']/(stats['tp']+stats['fn'])\n",
    "\n",
    "F = 2*precision*recall/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print precision, recall, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alias_to_entity['screenwriter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment_alias(tweets, alias_mapper):\n",
    "    length_sum = 0\n",
    "    match = 0\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    for tweet in tweets.values():\n",
    "        goldens = tweet['goldens']\n",
    "        for g in goldens:\n",
    "            total += 1\n",
    "            mention = g['mention']\n",
    "            real_mention = mention\n",
    "            \n",
    "            if mention in tweet['cashtag_mapping']:\n",
    "                real_mention = tweet['cashtag_mapping'][mention]['text']\n",
    "            elif mention in tweet['hashtag_mapping']:\n",
    "                real_mention = tweet['hashtag_mapping'][mention]['text']\n",
    "            elif mention in tweet['url_mapping']:\n",
    "                real_mention = tweet['url_mapping'][mention]['url']\n",
    "            elif mention in tweet['usermention_mapping']:\n",
    "                real_mention = tweet['usermention_mapping'][mention]['name']\n",
    "            \n",
    "            low = real_mention.lower()\n",
    "            if low in alias_mapper:\n",
    "                match += 1\n",
    "                length_sum += len(alias_mapper[low])\n",
    "            else:\n",
    "                print tweet['tweet_info']['id'], real_mention, \"|\", g['wiki_title']\n",
    "    print total, match, hit, length_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alias_to_entity_lower = {}\n",
    "for als in alias_to_entity.keys():\n",
    "    als_lower = als.lower()\n",
    "    if als_lower in alias_to_entity_lower:\n",
    "        alias_to_entity_lower[als_lower] |= alias_to_entity[als]\n",
    "    else:\n",
    "        alias_to_entity_lower[als_lower] = alias_to_entity[als]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_alias(tweet_corpus, alias_to_entity_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in alias_to_entity_lower:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "15124./1981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exact: 2420 1800    \n",
    "lower_case: 2420 1981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
