{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 align=\"center\">Data Preparation</h1> \n",
    " <h4 align=\"center\">Jiarui Xu - jxu57@illinois.edu</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"/Volumes/backup/ccg_tweet_wikifier_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Acquire Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set API keys\n",
    "consumer_key = 'FTglCRcahFJxH3U0TVoakD3BS'\n",
    "consumer_secret = 'BoNfyAic70hWkmH6jIzr4xoE48iFyIRnqvwE4NjSwpfJVwD7N2'\n",
    "access_token = '3081709104-XeNjGf7h9l6G1ERuPE1l9KOX85XJcWMBflWd1P8'\n",
    "access_token_secret = '9Z1czrN0jU4hs08KtxE7IeIcjNWAd1LbWqblvyvzJoRfN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up API\n",
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Load File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label-trainDev.tsv -- train split of NEEL [2] dataset    \n",
    "label-train.tsv -- train split of label-trainDev.tsv used in this paper (used for parameter tuning)    \n",
    "label-dev.tsv -- dev split of label-trainDev.tsv used in this paper (used for parameter tuning)    \n",
    "label-test.tsv -- test split of NEEL [2] dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list = [\"label-trainDev.tsv\", \"label-train.tsv\", \"label-dev.tsv\", \"label-test.tsv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a set of unique tweet ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_ids = set()\n",
    "for file_name in file_list:\n",
    "    with open (\"./data/v1-NEELOnly/\" + file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            segs = line.split(\"\\t\")\n",
    "            tweet_id = segs[0]\n",
    "            tweet_ids.add(tweet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Retrieve Tweets using API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to retrieve tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_tweet_set(id_list):\n",
    "    # set up progress bar\n",
    "    bar = pyprind.ProgBar(len(id_list))\n",
    "    \n",
    "    tweet_set = {}\n",
    "    failed = {}\n",
    "    \n",
    "    for id in id_list:\n",
    "        bar.update()\n",
    "        \n",
    "        try:\n",
    "            tweet_set[id] = api.get_status(id,)._json\n",
    "        except:\n",
    "            failed[id] = sys.exc_info()[0]\n",
    "    \n",
    "    return (tweet_set, failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = fill_tweet_set(tweet_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the tweet set to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(raw).pickle\", \"wb\") as f:\n",
    "    pickle.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_corpus = {}\n",
    "\n",
    "for tweet_id in result[0].keys():\n",
    "    tweet_corpus[tweet_id] = {}\n",
    "    tweet_corpus[tweet_id]['tweet_info'] = result[0][tweet_id]\n",
    "\n",
    "with open(data_folder+\"Tweet/NEEL_tweets(initial).pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweet_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample tweet object in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_corpus['93314579924393984']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Golden Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(initial).pickle\", \"rb\") as f:\n",
    "    tweet_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def golden_generator(tid, s_idx, e_idx, wiki_title, freebase, mention):\n",
    "    ret = {}\n",
    "    ret['tid'] = tid\n",
    "    ret['start_idx'] = s_idx\n",
    "    ret['end_idx'] = e_idx\n",
    "    ret['wiki_title'] = wiki_title\n",
    "    ret['freebase_title'] = freebase\n",
    "    ret['mention'] = mention\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def golden_standard(file_list, tweet_corpus):\n",
    "    unique_lines= set()\n",
    "    \n",
    "    for file_name in file_list: \n",
    "        with open (\"./data/v1-NEELOnly/\" + file_name, \"r\") as f:\n",
    "            for line in f:\n",
    "                unique_lines.add(line.strip())\n",
    "    \n",
    "    for tw in tweet_corpus.values():\n",
    "        tw['goldens'] = []\n",
    "        \n",
    "    for line in unique_lines:\n",
    "        segments = line.strip().split(\"\\t\")\n",
    "        tid = segments[0]\n",
    "        if tid in tweet_corpus:\n",
    "            golden = golden_generator(segments[0], segments[1], segments[2],\\\n",
    "                                        segments[3], segments[4], segments[5])\n",
    "            tweet_corpus[tid]['goldens'].append(golden)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loading ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "golden_standard(file_list, tweet_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweet_corpus.values():\n",
    "    text = tweet['tweet_info']['text']\n",
    "    for g in tweet['goldens']:\n",
    "        g['mention_orig'] = text.encode('utf-8')[int(g['start_idx']):int(g['end_idx'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of golden standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end_idx': '10',\n",
       "  'freebase_title': '/m/02_nkp',\n",
       "  'mention': 'Lamar Odom',\n",
       "  'mention_orig': 'Lamar Odom',\n",
       "  'start_idx': '0',\n",
       "  'tid': '93314579924393984',\n",
       "  'wiki_title': 'Lamar_Odom'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['93314579924393984']['goldens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting Twitter's Special Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://support.twitter.com/articles/166337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'indices': [14, 26], u'text': u'TheBachelor'}]\n"
     ]
    }
   ],
   "source": [
    "print tweet_corpus['93141776474456064']['tweet_info']['entities']['hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hashtag_mapping(tweet):\n",
    "    ret = {}\n",
    "    for tag in tweet['tweet_info']['entities']['hashtags']:\n",
    "        ret[tag['text']] = tag\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. User Mention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'indices': [113, 128], u'id_str': u'17890396', u'screen_name': u'chrisbharrison', u'name': u'Chris Harrison', u'id': 17890396}]\n"
     ]
    }
   ],
   "source": [
    "print tweet_corpus['93141776474456064']['tweet_info']['entities']['user_mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_usermention_mapping(tweet):\n",
    "    ret = {}\n",
    "    for name in tweet['tweet_info']['entities']['user_mentions']:\n",
    "        ret[name['screen_name']] = name\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Cashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'indices': [105, 110], u'text': u'AAPL'}]\n"
     ]
    }
   ],
   "source": [
    "print tweet_corpus['100982604374880256']['tweet_info']['entities']['symbols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cashtag_mapping(tweet):\n",
    "    ret = {}\n",
    "    for tag in tweet['tweet_info']['entities']['symbols']:\n",
    "        ret[tag['text']] = tag\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'url': u'http://t.co/vOtnNjD', u'indices': [85, 104], u'expanded_url': u'http://reut.rs/pJSj5o', u'display_url': u'reut.rs/pJSj5o'}]\n"
     ]
    }
   ],
   "source": [
    "print tweet_corpus['100982604374880256']['tweet_info']['entities']['urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url_mapping(tweet):\n",
    "    ret = {}\n",
    "    for url in tweet['tweet_info']['entities']['urls']:\n",
    "        ret[url['url']] = url\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate throught the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in tweet_corpus.values():\n",
    "    tweet['hashtag_mapping'] = make_hashtag_mapping(tweet)\n",
    "    tweet['usermention_mapping'] = make_usermention_mapping(tweet)\n",
    "    tweet['cashtag_mapping'] = make_cashtag_mapping(tweet)\n",
    "    tweet['url_mapping'] = make_url_mapping(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(with_tw).pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweet_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(with_tw).pickle\", \"rb\") as f:\n",
    "    tweet_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Text Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Stanford NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Models: 5/5                                                            \n"
     ]
    }
   ],
   "source": [
    "from corenlp import *\n",
    "corenlp = StanfordCoreNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stanford_parse(corenlp, text):\n",
    "    \"\"\"\n",
    "    transform the output into __dict__\n",
    "    \"\"\"\n",
    "    return json.loads(corenlp.parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                                                                  100%\n",
      "[######################################################################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:10:38\n"
     ]
    }
   ],
   "source": [
    "bar = pyprind.ProgBar(len(tweet_corpus), width = 70)\n",
    "\n",
    "for tweet in tweet_corpus.values():\n",
    "    bar.update()\n",
    "    text = tweet['tweet_info']['text']\n",
    "    \n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "\n",
    "    # tweet['stanford_parsed_text'] = corenlp.parse(text)\n",
    "    tweet['stanford_parsed'] = stanford_parse(corenlp, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = tweet_corpus['94518623552552961']['stanford_parsed_text'].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Police: The Ut\\xf8ya shooter is a 32 year old Norwegian national. His ethnicity has NOT been disclosed. Interrogation is currently taking place'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['94518623552552961']['tweet_info']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Police:',\n",
       " u': The',\n",
       " u'The Ut\\xf8ya',\n",
       " u'Ut\\xf8ya shooter',\n",
       " u'shooter is',\n",
       " u'is a',\n",
       " u'a 32',\n",
       " u'32 year',\n",
       " u'year old',\n",
       " u'old Norwegian',\n",
       " u'Norwegian national',\n",
       " u'national.',\n",
       " u'His ethnicity',\n",
       " u'ethnicity has',\n",
       " u'has NOT',\n",
       " u'NOT been',\n",
       " u'been disclosed',\n",
       " u'disclosed.',\n",
       " u'Interrogation is',\n",
       " u'is currently',\n",
       " u'currently taking',\n",
       " u'taking place']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['94518623552552961']['ngrams'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end_idx': '25',\n",
       "  'freebase_title': '/m/0h1fpkg',\n",
       "  'mention': 'Ut\\xc3\\xb8ya shooter',\n",
       "  'start_idx': '12',\n",
       "  'tid': '94518623552552961',\n",
       "  'wiki_title': 'Anders_Behring_Breivik'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['94518623552552961']['goldens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Police: The Ut\\xf8ya shooter is a 32 year old Norwegian national. His ethnicity has NOT been disclosed. Interrogation is currently taking place'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['94518623552552961']['tweet_info']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. N-gram generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def remove_zero(inlist):\n",
    "    for ele in inlist:\n",
    "        if len(ele) == 0:\n",
    "            inlist.remove(ele)\n",
    "\n",
    "def gram_to_string(segments, sentence_text):\n",
    "    \"\"\"\n",
    "    return the surface givan n_gram\n",
    "    \"\"\"\n",
    "    start_idx = segments[0][1]['CharacterOffsetBegin']\n",
    "    end_idx = segments[-1][1]['CharacterOffsetEnd']\n",
    "    return sentence_text[int(start_idx): int(end_idx)].encode('UTF-8')\n",
    "    \n",
    "def n_grams_helper(parsed, n, sentence_text):\n",
    "    \"\"\"\n",
    "    wrapper for previous function\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "    \n",
    "    text = parsed['text']\n",
    "    words = parsed['words']\n",
    "    length = len(words)\n",
    "    \n",
    "    if length<n:\n",
    "        return []\n",
    "    \n",
    "    for idx in range(0, length-n+1):\n",
    "        segments = words[idx : idx+n]\n",
    "        ret.append(gram_to_string(segments,sentence_text))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_grams_generator(stanford_core, text, max_n, parsed):\n",
    "    grams = {}\n",
    "\n",
    "    # parsed = stanford_parse(stanford_core, text)\n",
    "    sentences = parsed['sentences']\n",
    "    \n",
    "    for n in range(1, max_n):\n",
    "        grams[n] = []\n",
    "        for sen in sentences:\n",
    "            grams[n].extend(n_grams_helper(sen, n, text))\n",
    "        remove_zero(grams[n])\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                                                                  100%\n",
      "[######################################################################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "bar = pyprind.ProgBar(len(tweet_corpus), width = 70)\n",
    "\n",
    "for tweet in tweet_corpus.values():\n",
    "    bar.update()\n",
    "    tweet['ngrams'] = n_grams_generator(corenlp, tweet['tweet_info']['text'],\\\n",
    "                                                    100, tweet['stanford_parsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_grams_reprocess:\n",
    "\n",
    "for tweet in tweet_corpus.values():\n",
    "    for n in tweet['ngrams'].keys():\n",
    "        for onegram in tweet['ngrams'][n]:\n",
    "            if onegram.count(\"/\") == 1:\n",
    "                if \"http\" in onegram:\n",
    "                    continue\n",
    "                segments = onegram.split(\"/\")\n",
    "                remove_zero(segments)\n",
    "                tweet['ngrams'][n].extend(segments)\n",
    "\n",
    "            if onegram.count(\"-\") == 1:\n",
    "                segments = onegram.split(\"-\")\n",
    "                remove_zero(segments)\n",
    "                tweet['ngrams'][n].extend(segments)\n",
    "\n",
    "            if onegram.count(\"?\") == 1:\n",
    "                segments = onegram.split(\"?\")\n",
    "                remove_zero(segments)\n",
    "                tweet['ngrams'][n].extend(segments)\n",
    "            \n",
    "            if onegram.count(\".\") == 1:\n",
    "                segments = onegram.split(\"?\")\n",
    "                remove_zero(segments)\n",
    "                tweet['ngrams'][n].extend(segments)\n",
    "                \n",
    "            remove_zero(tweet['ngrams'][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_folder+\"Tweet/NEEL_tweets(with_grams).pickle\", \"wb\") as f:\n",
    "    pickle.dump(tweet_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['I',\n",
       "  'put',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Full',\n",
       "  'Armor',\n",
       "  'of',\n",
       "  'God',\n",
       "  '-',\n",
       "  'Eph',\n",
       "  '6:10'],\n",
       " 2: ['I put',\n",
       "  'put on',\n",
       "  'on the',\n",
       "  'the Full',\n",
       "  'Full Armor',\n",
       "  'Armor of',\n",
       "  'of God',\n",
       "  'God -',\n",
       "  '- Eph',\n",
       "  'Eph 6:10'],\n",
       " 3: ['I put on',\n",
       "  'put on the',\n",
       "  'on the Full',\n",
       "  'the Full Armor',\n",
       "  'Full Armor of',\n",
       "  'Armor of God',\n",
       "  'of God -',\n",
       "  'God - Eph',\n",
       "  '- Eph 6:10'],\n",
       " 4: ['I put on the',\n",
       "  'put on the Full',\n",
       "  'on the Full Armor',\n",
       "  'the Full Armor of',\n",
       "  'Full Armor of God',\n",
       "  'Armor of God -',\n",
       "  'of God - Eph',\n",
       "  'God - Eph 6:10'],\n",
       " 5: ['I put on the Full',\n",
       "  'put on the Full Armor',\n",
       "  'on the Full Armor of',\n",
       "  'the Full Armor of God',\n",
       "  'Full Armor of God -',\n",
       "  'Armor of God - Eph',\n",
       "  'of God - Eph 6:10'],\n",
       " 6: ['I put on the Full Armor',\n",
       "  'put on the Full Armor of',\n",
       "  'on the Full Armor of God',\n",
       "  'the Full Armor of God -',\n",
       "  'Full Armor of God - Eph',\n",
       "  'Armor of God - Eph 6:10'],\n",
       " 7: ['I put on the Full Armor of',\n",
       "  'put on the Full Armor of God',\n",
       "  'on the Full Armor of God -',\n",
       "  'the Full Armor of God - Eph',\n",
       "  'Full Armor of God - Eph 6:10'],\n",
       " 8: ['I put on the Full Armor of God',\n",
       "  'put on the Full Armor of God -',\n",
       "  'on the Full Armor of God - Eph',\n",
       "  'the Full Armor of God - Eph 6:10'],\n",
       " 9: ['I put on the Full Armor of God -',\n",
       "  'put on the Full Armor of God - Eph',\n",
       "  'on the Full Armor of God - Eph 6:10'],\n",
       " 10: ['I put on the Full Armor of God - Eph',\n",
       "  'put on the Full Armor of God - Eph 6:10'],\n",
       " 11: ['I put on the Full Armor of God - Eph 6:10'],\n",
       " 12: [],\n",
       " 13: [],\n",
       " 14: [],\n",
       " 15: [],\n",
       " 16: [],\n",
       " 17: [],\n",
       " 18: [],\n",
       " 19: [],\n",
       " 20: [],\n",
       " 21: [],\n",
       " 22: [],\n",
       " 23: [],\n",
       " 24: [],\n",
       " 25: [],\n",
       " 26: [],\n",
       " 27: [],\n",
       " 28: [],\n",
       " 29: [],\n",
       " 30: [],\n",
       " 31: [],\n",
       " 32: [],\n",
       " 33: [],\n",
       " 34: [],\n",
       " 35: [],\n",
       " 36: [],\n",
       " 37: [],\n",
       " 38: [],\n",
       " 39: [],\n",
       " 40: [],\n",
       " 41: [],\n",
       " 42: [],\n",
       " 43: [],\n",
       " 44: [],\n",
       " 45: [],\n",
       " 46: [],\n",
       " 47: [],\n",
       " 48: [],\n",
       " 49: [],\n",
       " 50: [],\n",
       " 51: [],\n",
       " 52: [],\n",
       " 53: [],\n",
       " 54: [],\n",
       " 55: [],\n",
       " 56: [],\n",
       " 57: [],\n",
       " 58: [],\n",
       " 59: [],\n",
       " 60: [],\n",
       " 61: [],\n",
       " 62: [],\n",
       " 63: [],\n",
       " 64: [],\n",
       " 65: [],\n",
       " 66: [],\n",
       " 67: [],\n",
       " 68: [],\n",
       " 69: [],\n",
       " 70: [],\n",
       " 71: [],\n",
       " 72: [],\n",
       " 73: [],\n",
       " 74: [],\n",
       " 75: [],\n",
       " 76: [],\n",
       " 77: [],\n",
       " 78: [],\n",
       " 79: [],\n",
       " 80: [],\n",
       " 81: [],\n",
       " 82: [],\n",
       " 83: [],\n",
       " 84: [],\n",
       " 85: [],\n",
       " 86: [],\n",
       " 87: [],\n",
       " 88: [],\n",
       " 89: [],\n",
       " 90: [],\n",
       " 91: [],\n",
       " 92: [],\n",
       " 93: [],\n",
       " 94: [],\n",
       " 95: [],\n",
       " 96: [],\n",
       " 97: [],\n",
       " 98: [],\n",
       " 99: []}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus['92955019615272961']['ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-6428fc7e38a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dasd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "m.remove(\"dasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sda', 'dsa']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
